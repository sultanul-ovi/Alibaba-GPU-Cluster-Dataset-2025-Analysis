{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e3ed88",
   "metadata": {},
   "source": [
    "# Detailed Analysis Traces for GPU-Disaggregated Deep Learning Recommendation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab051c50",
   "metadata": {},
   "source": [
    "## â„¹ï¸ Overview\n",
    "\n",
    "This repository contains a comprehensive trace dataset for GPU-disaggregated serving of Deep Learning Recommendation Models (DLRMs).\n",
    "The dataset captures operational characteristics of **156 inference services**, comprising a total of **23,871 inference instances**. \n",
    "These instances are further divided into **16,485 CN (CPU Node) inference instances** and **7,386 HN (Heterogeneous GPU Node) inference instances**.\n",
    "\n",
    "All instances in this dataset are categorized as *Latency-Sensitive (LS)* workloads, reflecting their critical performance requirements. These inference instances are typically **high-priority** and **long-running**, ensuring sustained availability and responsiveness for end users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb70b93",
   "metadata": {},
   "source": [
    "## ðŸ—„ï¸ Dataset Details\n",
    "\n",
    "The core dataset is provided in the file [`disaggregated_DLRM_trace.csv`](./disaggregated_DLRM_trace.csv).\n",
    "\n",
    "### Field Descriptions\n",
    "\n",
    "- `instance_sn`: Unique identifier for the instance.\n",
    "- `role`: Role of the instance.\n",
    "  - `CN`: CPU Node\n",
    "  - `HN`: Heterogeneous GPU Node\n",
    "- `app_name`: Name of the application group to which the instance belongs. An application group is a collection of instances sharing the same application name. For example, `app_0` may contain multiple instances like `instance_0`, `instance_1`, etc.\n",
    "- `cpu_request`: Number of CPU cores requested by the instance (in vCPUs).\n",
    "- `cpu_limit`: Maximum number of CPU cores allowed for the instance (same as `cpu_request` in this scenario).\n",
    "- `gpu_request`: Number of GPUs requested by the instance.\n",
    "- `gpu_limit`: Maximum number of GPUs allowed for the instance (same as `gpu_request` in this scenario).\n",
    "- `rdma_request`: Allocated percentage of the bandwidth of an RDMA Network Interface Card (RNIC), ranging from 0 to 100. Currently, this value is used as a constraint for scheduling density.\n",
    "- `rdma_limit`: Maximum RDMA bandwidth allowed for the instance (same as `rdma_request` in this scenario).\n",
    "- `memory_request`: Amount of main memory requested by the instance (in GiB).\n",
    "- `memory_limit`: Maximum amount of main memory allowed for the instance (in GiB).\n",
    "- `disk_request`: Amount of disk space requested by the instance (in GiB).\n",
    "- `disk_limit`: Maximum amount of disk space allowed for the instance (in GiB).\n",
    "- `max_instance_per_node`: Maximum number of instances of the same `app_name` that can be deployed on a single node. A value of `-1` indicates no deployment density limit.\n",
    "- `creation_time`: Timestamp indicating when the instance was created, expressed as the difference in **seconds** from the trace start time. If the instance existed before the trace start time, this field is set to `NaN`.\n",
    "- `scheduled_time`: Timestamp indicating when the instance was scheduled, expressed as the difference in **seconds** from the trace start time. If the instance was scheduled before the trace start time, this field is set to `NaN`.\n",
    "- `deletion_time`: Timestamp indicating when the instance was deleted, expressed as the difference in seconds from the trace start time. If the instance was deleted after the trace end time, this field is set to `NaN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79d8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the disaggregated DLRM trace CSV, run detailed analysis, and save figures, tables, and an HTML report.\n",
    "\n",
    "\"\"\"\n",
    "How to use\n",
    "    Put this script next to disaggregated_DLRM_trace.csv.\n",
    "    Run it with a regular Python interpreter. No arguments needed.\n",
    "\n",
    "What you get in ./dlrm_analysis_out\n",
    "    CSV tables\n",
    "        - overall_resource_stats.csv\n",
    "        - per_role_resource_stats.csv\n",
    "        - correlation_matrix.csv\n",
    "        - per_app_aggregates.csv\n",
    "        - max_instance_per_node_counts.csv\n",
    "        - time_summaries.csv\n",
    "        - concurrency_over_time.csv\n",
    "        - arrivals_per_hour.csv\n",
    "        - departures_per_hour.csv\n",
    "        - per_app_time_stats.csv\n",
    "    Figures in ./dlrm_analysis_out/figures\n",
    "        - histograms for CPU, GPU, memory, RDMA, disk, schedule_delay, runtime\n",
    "        - boxplots of CPU by role and memory by role\n",
    "        - heatmap of resource correlation\n",
    "        - concurrency_over_time.png\n",
    "        - arrivals_and_departures_over_time.png\n",
    "        - cpu_vs_memory_scatter.png\n",
    "        - rdma_distribution_by_role.png\n",
    "    A single HTML report\n",
    "        - report.html that links to every figure and table\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0174c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- configuration ---------------------------\n",
    "\n",
    "OUTDIR = Path(\"./dlrm_analysis_out\")\n",
    "FIGDIR = OUTDIR / \"figures\"\n",
    "CSV_NAME = \"disaggregated_DLRM_trace.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc35000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- utilities ---------------------------\n",
    "\n",
    "def ensure_outdirs(outdir: Path, figdir: Path) -> None:\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    figdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def to_numeric(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def describe_resources(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\"cpu_request\", \"gpu_request\", \"rdma_request\", \"memory_request\", \"disk_request\"]\n",
    "    cols = [c for c in cols if c in frame.columns]\n",
    "    desc = frame[cols].describe(percentiles=[0.5, 0.9, 0.95]).T\n",
    "    desc = desc.rename(columns={\"50%\": \"p50\", \"90%\": \"p90\", \"95%\": \"p95\"})\n",
    "    return desc[[\"count\", \"mean\", \"std\", \"min\", \"p50\", \"p90\", \"p95\", \"max\"]]\n",
    "\n",
    "def safe_percentile(x: pd.Series, q: float) -> float:\n",
    "    x = pd.to_numeric(x, errors=\"coerce\").dropna()\n",
    "    if len(x) == 0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.percentile(x, q))\n",
    "\n",
    "def seconds_to_hms(sec: float) -> str:\n",
    "    if not math.isfinite(sec):\n",
    "        return \"na\"\n",
    "    sec = int(round(sec))\n",
    "    d, rem = divmod(sec, 86400)\n",
    "    h, rem = divmod(rem, 3600)\n",
    "    m, s = divmod(rem, 60)\n",
    "    if d > 0:\n",
    "        return f\"{d}d {h}h {m}m {s}s\"\n",
    "    if h > 0:\n",
    "        return f\"{h}h {m}m {s}s\"\n",
    "    if m > 0:\n",
    "        return f\"{m}m {s}s\"\n",
    "    return f\"{s}s\"\n",
    "\n",
    "def save_hist(series: pd.Series, title: str, xlabel: str, figpath: Path, bins=30, logx=False, logy=False):\n",
    "    data = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if len(data) == 0:\n",
    "        return\n",
    "    plt.figure()\n",
    "    if logx:\n",
    "        positive = data[data > 0]\n",
    "        if len(positive) > 0:\n",
    "            edges = np.logspace(np.log10(positive.min()), np.log10(positive.max()), bins + 1)\n",
    "            plt.hist(positive, bins=edges)\n",
    "            plt.xscale(\"log\")\n",
    "        else:\n",
    "            plt.hist(data, bins=bins)\n",
    "    else:\n",
    "        plt.hist(data, bins=bins)\n",
    "    if logy:\n",
    "        plt.yscale(\"log\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figpath, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "def save_box_by_role(df: pd.DataFrame, col: str, figpath: Path):\n",
    "    sub = df[[\"role\", col]].dropna()\n",
    "    if sub.empty:\n",
    "        return\n",
    "    plt.figure()\n",
    "    sub.boxplot(by=\"role\", column=[col])\n",
    "    plt.suptitle(\"\")\n",
    "    plt.title(f\"{col} by role\")\n",
    "    plt.xlabel(\"role\")\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figpath, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "def save_heatmap(corr: pd.DataFrame, title: str, figpath: Path):\n",
    "    if corr.empty:\n",
    "        return\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(corr.values, interpolation=\"nearest\")\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(corr.index)), corr.index)\n",
    "    for i in range(len(corr.index)):\n",
    "        for j in range(len(corr.columns)):\n",
    "            v = corr.values[i, j]\n",
    "            plt.text(j, i, f\"{v:.2f}\", ha=\"center\", va=\"center\", fontsize=8, color=\"white\")\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figpath, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "def sweep_concurrency(starts: np.ndarray, ends: np.ndarray, tmin: float, tmax: float, hour_bin: float = 1.0):\n",
    "    if not np.isfinite(tmin) or not np.isfinite(tmax) or tmax <= tmin:\n",
    "        return pd.Series(dtype=float)\n",
    "    step = 3600.0 * hour_bin\n",
    "    n_bins = int(math.ceil((tmax - tmin) / step)) + 1\n",
    "    edges = np.linspace(tmin, tmin + n_bins * step, n_bins + 1)\n",
    "    centers = (edges[:-1] + edges[1:]) / 2.0\n",
    "    sh, _ = np.histogram(starts, bins=edges)\n",
    "    eh, _ = np.histogram(ends, bins=edges)\n",
    "    conc = np.cumsum(sh) - np.cumsum(eh)\n",
    "    return pd.Series(conc, index=pd.Index(centers, name=\"time_sec\"), name=\"concurrency\")\n",
    "\n",
    "def hourly_counts(ts: pd.Series, tmin: float, tmax: float):\n",
    "    if ts.dropna().empty or not np.isfinite(tmin) or not np.isfinite(tmax) or tmax <= tmin:\n",
    "        return pd.Series(dtype=float)\n",
    "    step = 3600.0\n",
    "    n_bins = int(math.ceil((tmax - tmin) / step)) + 1\n",
    "    edges = np.linspace(tmin, tmin + n_bins * step, n_bins + 1)\n",
    "    centers = (edges[:-1] + edges[1:]) / 2.0\n",
    "    counts, _ = np.histogram(ts.dropna().values, bins=edges)\n",
    "    return pd.Series(counts, index=pd.Index(centers, name=\"time_sec\"))\n",
    "\n",
    "def scatter_cpu_mem(df: pd.DataFrame, figpath: Path):\n",
    "    sub = df[[\"cpu_request\", \"memory_request\", \"role\"]].dropna()\n",
    "    if sub.empty:\n",
    "        return\n",
    "    plt.figure()\n",
    "    for role, g in sub.groupby(\"role\"):\n",
    "        plt.scatter(g[\"cpu_request\"], g[\"memory_request\"], s=6, alpha=0.6, label=role)\n",
    "    plt.xlabel(\"CPU request vCPUs\")\n",
    "    plt.ylabel(\"Memory request GiB\")\n",
    "    plt.legend()\n",
    "    plt.title(\"CPU versus memory request\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figpath, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "def rdma_hist_by_role(df: pd.DataFrame, figpath: Path):\n",
    "    sub = df[[\"role\", \"rdma_request\"]].dropna()\n",
    "    if sub.empty:\n",
    "        return\n",
    "    plt.figure()\n",
    "    roles = sorted(sub[\"role\"].dropna().unique())\n",
    "    for idx, role in enumerate(roles):\n",
    "        vals = sub.loc[sub[\"role\"] == role, \"rdma_request\"]\n",
    "        plt.hist(vals, bins=30, alpha=0.5, label=role)\n",
    "    plt.xlabel(\"RDMA percent\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"RDMA request distribution by role\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figpath, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "def write_report_html(context: Dict, outpath: Path):\n",
    "    def link(p: Path) -> str:\n",
    "        rel = os.path.relpath(p, outpath.parent)\n",
    "        return f'<a href=\"{rel}\" target=\"_blank\">{p.name}</a>'\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    lines = []\n",
    "    lines.append(\"<html><head><meta charset='utf-8'><title>DLRM trace analysis</title></head><body>\")\n",
    "    lines.append(f\"<h2>DLRM trace analysis</h2><p>Generated at {now}</p>\")\n",
    "    lines.append(\"<h3>Dataset scope</h3>\")\n",
    "    lines.append(\"<ul>\")\n",
    "    lines.append(f\"<li>Rows {context['n_rows']}, columns {context['n_cols']}, apps {context['n_apps']}</li>\")\n",
    "    lines.append(f\"<li>Role split, CN {context['cn']} and HN {context['hn']}</li>\")\n",
    "    lines.append(f\"<li>GPU backed share {context['gpu_share']:.2%}</li>\")\n",
    "    lines.append(\"</ul>\")\n",
    "\n",
    "    lines.append(\"<h3>Key medians by role</h3>\")\n",
    "    lines.append(\"<ul>\")\n",
    "    lines.append(f\"<li>CN median, CPU {context['cn_cpu_med']:.0f} vCPUs, memory {context['cn_mem_med']:.0f} GiB, RDMA {context['cn_rdma_med']:.0f} percent</li>\")\n",
    "    lines.append(f\"<li>HN median, CPU {context['hn_cpu_med']:.0f} vCPUs, memory {context['hn_mem_med']:.0f} GiB, GPU {context['hn_gpu_med']:.0f}, RDMA {context['hn_rdma_med']:.0f} percent</li>\")\n",
    "    lines.append(\"</ul>\")\n",
    "\n",
    "    lines.append(\"<h3>Correlations</h3>\")\n",
    "    lines.append(f\"<p>CPU to memory {context['corr_cpu_mem']:.3f}, GPU to RDMA {context['corr_gpu_rdma']:.3f}</p>\")\n",
    "\n",
    "    lines.append(\"<h3>Density and placement</h3>\")\n",
    "    lines.append(f\"<p>Mode of max_instance_per_node {context['ipn_mode']}, share with no limit {context['ipn_no_limit']:.2%}</p>\")\n",
    "\n",
    "    lines.append(\"<h3>Timing behavior</h3>\")\n",
    "    lines.append(\"<ul>\")\n",
    "    lines.append(f\"<li>Schedule delay p50 {context['sched_p50']}, p90 {context['sched_p90']}, p95 {context['sched_p95']}</li>\")\n",
    "    lines.append(f\"<li>Runtime p50 {context['run_p50']}, p90 {context['run_p90']}, p95 {context['run_p95']}</li>\")\n",
    "    lines.append(f\"<li>Missing counts, creation NaN {context['preexist']}, scheduled NaN {context['prescheduled']}, deletion NaN {context['posttrace']}</li>\")\n",
    "    lines.append(\"</ul>\")\n",
    "\n",
    "    lines.append(\"<h3>Figures</h3><ul>\")\n",
    "    for fig in context[\"figs\"]:\n",
    "        lines.append(f\"<li>{link(fig)}</li>\")\n",
    "    lines.append(\"</ul>\")\n",
    "\n",
    "    lines.append(\"<h3>Tables</h3><ul>\")\n",
    "    for tbl in context[\"tables\"]:\n",
    "        lines.append(f\"<li>{link(tbl)}</li>\")\n",
    "    lines.append(\"</ul>\")\n",
    "\n",
    "    lines.append(\"</body></html>\")\n",
    "    outpath.write_text(\"\\n\".join(lines), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40a8662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Disaggregated DLRM trace summary ==========\n",
      "Rows 23,871   Columns 17   Apps 156\n",
      "Role split, CN 16,485   HN 7,386   GPU backed share 30.94%\n",
      "CN median, CPU 64 vCPUs   Memory 320 GiB   RDMA 1 percent\n",
      "HN median, CPU 8 vCPUs   Memory 40 GiB   GPU 1   RDMA 25 percent\n",
      "Corr CPU to memory 0.987   Corr GPU to RDMA 0.173\n",
      "Mode of max_instance_per_node -1   Share with no limit 51.31%\n",
      "Schedule delay p50 0s   p90 59s   p95 2m 16s\n",
      "Runtime p50 2h 17m 58s   p90 3d 7h 39m 47s   p95 7d 1h 50m 50s\n",
      "Missing counts, creation NaN 7,280   scheduled NaN 7,280   deletion NaN 8,878\n",
      "Concurrency samples 745 from tmin 75 to tmax 2677541 seconds\n",
      "Outputs are in /Users/sultanulovi/Downloads/clusterdata-master/cluster-trace-gpu-v2025/dlrm_analysis_out\n",
      "======================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------- main workflow ---------------------------\n",
    "\n",
    "def main():\n",
    "    ensure_outdirs(OUTDIR, FIGDIR)\n",
    "\n",
    "    if not Path(CSV_NAME).exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find {CSV_NAME}. Place the CSV next to this script.\"\n",
    "        )\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(CSV_NAME)\n",
    "\n",
    "    # Convert numeric columns\n",
    "    num_cols = [\n",
    "        \"cpu_request\", \"cpu_limit\", \"gpu_request\", \"gpu_limit\",\n",
    "        \"rdma_request\", \"rdma_limit\", \"memory_request\", \"memory_limit\",\n",
    "        \"disk_request\", \"disk_limit\", \"max_instance_per_node\",\n",
    "        \"creation_time\", \"scheduled_time\", \"deletion_time\",\n",
    "    ]\n",
    "    to_numeric(df, num_cols)\n",
    "\n",
    "    # Basic shape\n",
    "    n_rows = len(df)\n",
    "    n_cols = len(df.columns)\n",
    "    n_apps = df[\"app_name\"].nunique()\n",
    "\n",
    "    # Role counts\n",
    "    role_counts = df[\"role\"].value_counts(dropna=False)\n",
    "    cn = int(role_counts.get(\"CN\", 0))\n",
    "    hn = int(role_counts.get(\"HN\", 0))\n",
    "\n",
    "    # Resource summaries\n",
    "    overall_stats = describe_resources(df)\n",
    "    by_role_stats = df.groupby(\"role\", dropna=False).apply(describe_resources)\n",
    "\n",
    "    # Correlation matrix\n",
    "    corr_cols = [\"cpu_request\", \"gpu_request\", \"rdma_request\", \"memory_request\", \"disk_request\"]\n",
    "    corr = df[corr_cols].corr()\n",
    "\n",
    "    # Density distribution\n",
    "    ipn_counts = df[\"max_instance_per_node\"].value_counts(dropna=False).sort_index()\n",
    "    ipn_mode = int(df[\"max_instance_per_node\"].mode().iloc[0]) if not df[\"max_instance_per_node\"].mode().empty else None\n",
    "    ipn_no_limit_share = float((df[\"max_instance_per_node\"] == -1).mean())\n",
    "\n",
    "    # Timing features\n",
    "    df[\"schedule_delay\"] = np.where(\n",
    "        df[\"creation_time\"].notna() & df[\"scheduled_time\"].notna(),\n",
    "        df[\"scheduled_time\"] - df[\"creation_time\"],\n",
    "        np.nan,\n",
    "    )\n",
    "    df[\"runtime\"] = np.where(\n",
    "        df[\"scheduled_time\"].notna() & df[\"deletion_time\"].notna(),\n",
    "        df[\"deletion_time\"] - df[\"scheduled_time\"],\n",
    "        np.nan,\n",
    "    )\n",
    "\n",
    "    sched_p50 = safe_percentile(df[\"schedule_delay\"], 50)\n",
    "    sched_p90 = safe_percentile(df[\"schedule_delay\"], 90)\n",
    "    sched_p95 = safe_percentile(df[\"schedule_delay\"], 95)\n",
    "    run_p50 = safe_percentile(df[\"runtime\"], 50)\n",
    "    run_p90 = safe_percentile(df[\"runtime\"], 90)\n",
    "    run_p95 = safe_percentile(df[\"runtime\"], 95)\n",
    "\n",
    "    preexisting = int(df[\"creation_time\"].isna().sum())\n",
    "    prescheduled = int(df[\"scheduled_time\"].isna().sum())\n",
    "    posttrace = int(df[\"deletion_time\"].isna().sum())\n",
    "\n",
    "    # Concurrency and hourly arrivals or departures\n",
    "    all_times = pd.concat([df[\"creation_time\"], df[\"scheduled_time\"], df[\"deletion_time\"]], ignore_index=True).dropna()\n",
    "    tmin = float(all_times.min()) if len(all_times) else 0.0\n",
    "    tmax = float(all_times.max()) if len(all_times) else 0.0\n",
    "\n",
    "    valid = df[\"scheduled_time\"].notna()\n",
    "    starts = df.loc[valid, \"scheduled_time\"].to_numpy()\n",
    "    ends = df.loc[valid, \"deletion_time\"].to_numpy()\n",
    "    ends_filled = np.where(np.isfinite(ends), ends, tmax)\n",
    "    conc_series = sweep_concurrency(starts, ends_filled, tmin, tmax, hour_bin=1.0)\n",
    "\n",
    "    arrivals_hourly = hourly_counts(df[\"scheduled_time\"], tmin, tmax)\n",
    "    departures_hourly = hourly_counts(df[\"deletion_time\"], tmin, tmax)\n",
    "\n",
    "    # Per app aggregates and time stats\n",
    "    per_app = df.groupby(\"app_name\").agg(\n",
    "        instances=(\"instance_sn\", \"nunique\"),\n",
    "        role_cn=(\"role\", lambda s: int((s == \"CN\").sum())),\n",
    "        role_hn=(\"role\", lambda s: int((s == \"HN\").sum())),\n",
    "        cpu_req_sum=(\"cpu_request\", \"sum\"),\n",
    "        gpu_req_sum=(\"gpu_request\", \"sum\"),\n",
    "        rdma_req_sum=(\"rdma_request\", \"sum\"),\n",
    "        mem_req_sum=(\"memory_request\", \"sum\"),\n",
    "        disk_req_sum=(\"disk_request\", \"sum\"),\n",
    "        max_ipn_mode=(\"max_instance_per_node\", lambda s: s.mode().iloc[0] if not s.mode().empty else np.nan),\n",
    "        sched_delay_p50=(\"schedule_delay\", lambda s: safe_percentile(s, 50)),\n",
    "        runtime_p50=(\"runtime\", lambda s: safe_percentile(s, 50)),\n",
    "    ).reset_index()\n",
    "\n",
    "    # --------------------------- save CSV tables ---------------------------\n",
    "\n",
    "    overall_stats.round(3).to_csv(OUTDIR / \"overall_resource_stats.csv\")\n",
    "    by_role_stats.round(3).to_csv(OUTDIR / \"per_role_resource_stats.csv\")\n",
    "    corr.round(3).to_csv(OUTDIR / \"correlation_matrix.csv\")\n",
    "    per_app.round(3).to_csv(OUTDIR / \"per_app_aggregates.csv\", index=False)\n",
    "    ipn_counts.to_csv(OUTDIR / \"max_instance_per_node_counts.csv\", header=[\"count\"])\n",
    "\n",
    "    time_df = pd.DataFrame({\n",
    "        \"schedule_delay_p50_s\": [sched_p50],\n",
    "        \"schedule_delay_p90_s\": [sched_p90],\n",
    "        \"schedule_delay_p95_s\": [sched_p95],\n",
    "        \"runtime_p50_s\": [run_p50],\n",
    "        \"runtime_p90_s\": [run_p90],\n",
    "        \"runtime_p95_s\": [run_p95],\n",
    "        \"preexisting_creation_na\": [preexisting],\n",
    "        \"prescheduled_na\": [prescheduled],\n",
    "        \"posttrace_deletion_na\": [posttrace],\n",
    "        \"tmin_s\": [tmin],\n",
    "        \"tmax_s\": [tmax],\n",
    "    })\n",
    "    time_df.to_csv(OUTDIR / \"time_summaries.csv\", index=False)\n",
    "\n",
    "    if not conc_series.empty:\n",
    "        conc_series.to_csv(OUTDIR / \"concurrency_over_time.csv\", header=True)\n",
    "    if not arrivals_hourly.empty:\n",
    "        arrivals_hourly.to_csv(OUTDIR / \"arrivals_per_hour.csv\", header=[\"arrivals\"])\n",
    "    if not departures_hourly.empty:\n",
    "        departures_hourly.to_csv(OUTDIR / \"departures_per_hour.csv\", header=[\"departures\"])\n",
    "\n",
    "    # --------------------------- save figures ---------------------------\n",
    "\n",
    "    save_hist(df[\"cpu_request\"], \"Distribution of CPU requests\", \"CPU request vCPUs\", FIGDIR / \"cpu_request_hist.png\", bins=40)\n",
    "    save_hist(df[\"gpu_request\"], \"Distribution of GPU requests\", \"GPU count\", FIGDIR / \"gpu_request_hist.png\", bins=5)\n",
    "    save_hist(df[\"memory_request\"], \"Distribution of memory requests\", \"Memory GiB\", FIGDIR / \"memory_request_hist.png\", bins=40)\n",
    "    save_hist(df[\"rdma_request\"], \"Distribution of RDMA requests\", \"RDMA percent\", FIGDIR / \"rdma_request_hist.png\", bins=40)\n",
    "    if \"disk_request\" in df.columns:\n",
    "        save_hist(df[\"disk_request\"], \"Distribution of disk requests\", \"Disk GiB\", FIGDIR / \"disk_request_hist.png\", bins=40)\n",
    "\n",
    "    save_hist(df[\"schedule_delay\"], \"Distribution of schedule delays\", \"Delay seconds\", FIGDIR / \"schedule_delay_hist.png\", bins=60, logx=True, logy=True)\n",
    "    save_hist(df[\"runtime\"], \"Distribution of runtimes\", \"Runtime seconds\", FIGDIR / \"runtime_hist.png\", bins=60, logx=True, logy=True)\n",
    "\n",
    "    save_box_by_role(df, \"cpu_request\", FIGDIR / \"cpu_by_role_box.png\")\n",
    "    save_box_by_role(df, \"memory_request\", FIGDIR / \"memory_by_role_box.png\")\n",
    "\n",
    "    save_heatmap(corr, \"Resource correlation\", FIGDIR / \"correlation_heatmap.png\")\n",
    "\n",
    "    if not conc_series.empty:\n",
    "        plt.figure()\n",
    "        conc_series.plot()\n",
    "        plt.xlabel(\"Trace time seconds\")\n",
    "        plt.ylabel(\"Concurrent active instances\")\n",
    "        plt.title(\"Active instance concurrency over time\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGDIR / \"concurrency_over_time.png\", dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "    if not arrivals_hourly.empty or not departures_hourly.empty:\n",
    "        plt.figure()\n",
    "        if not arrivals_hourly.empty:\n",
    "            arrivals_hourly.plot(label=\"arrivals\")\n",
    "        if not departures_hourly.empty:\n",
    "            departures_hourly.plot(label=\"departures\")\n",
    "        plt.xlabel(\"Trace time seconds\")\n",
    "        plt.ylabel(\"Count per hour\")\n",
    "        plt.title(\"Arrivals and departures per hour\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGDIR / \"arrivals_and_departures_over_time.png\", dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "    scatter_cpu_mem(df, FIGDIR / \"cpu_vs_memory_scatter.png\")\n",
    "    rdma_hist_by_role(df, FIGDIR / \"rdma_distribution_by_role.png\")\n",
    "\n",
    "    # --------------------------- console summary ---------------------------\n",
    "\n",
    "    gpu_share = float((df.get(\"gpu_request\", pd.Series(dtype=float)) > 0).mean()) if \"gpu_request\" in df.columns else 0.0\n",
    "\n",
    "    cn_cpu_med = float(df.loc[df[\"role\"] == \"CN\", \"cpu_request\"].median())\n",
    "    cn_mem_med = float(df.loc[df[\"role\"] == \"CN\", \"memory_request\"].median())\n",
    "    cn_rdma_med = float(df.loc[df[\"role\"] == \"CN\", \"rdma_request\"].median())\n",
    "\n",
    "    hn_cpu_med = float(df.loc[df[\"role\"] == \"HN\", \"cpu_request\"].median())\n",
    "    hn_mem_med = float(df.loc[df[\"role\"] == \"HN\", \"memory_request\"].median())\n",
    "    hn_gpu_med = float(df.loc[df[\"role\"] == \"HN\", \"gpu_request\"].median())\n",
    "    hn_rdma_med = float(df.loc[df[\"role\"] == \"HN\", \"rdma_request\"].median())\n",
    "\n",
    "    corr_cpu_mem = float(corr.loc[\"cpu_request\", \"memory_request\"]) if {\"cpu_request\", \"memory_request\"} <= set(corr.index) else float(\"nan\")\n",
    "    corr_gpu_rdma = float(corr.loc[\"gpu_request\", \"rdma_request\"]) if {\"gpu_request\", \"rdma_request\"} <= set(corr.index) else float(\"nan\")\n",
    "\n",
    "    print(\"\\n========== Disaggregated DLRM trace summary ==========\")\n",
    "    print(f\"Rows {n_rows:,}   Columns {n_cols:,}   Apps {n_apps:,}\")\n",
    "    print(f\"Role split, CN {cn:,}   HN {hn:,}   GPU backed share {gpu_share:.2%}\")\n",
    "    print(f\"CN median, CPU {cn_cpu_med:.0f} vCPUs   Memory {cn_mem_med:.0f} GiB   RDMA {cn_rdma_med:.0f} percent\")\n",
    "    print(f\"HN median, CPU {hn_cpu_med:.0f} vCPUs   Memory {hn_mem_med:.0f} GiB   GPU {hn_gpu_med:.0f}   RDMA {hn_rdma_med:.0f} percent\")\n",
    "    print(f\"Corr CPU to memory {corr_cpu_mem:.3f}   Corr GPU to RDMA {corr_gpu_rdma:.3f}\")\n",
    "    print(f\"Mode of max_instance_per_node {ipn_mode}   Share with no limit {ipn_no_limit_share:.2%}\")\n",
    "    print(f\"Schedule delay p50 {seconds_to_hms(sched_p50)}   p90 {seconds_to_hms(sched_p90)}   p95 {seconds_to_hms(sched_p95)}\")\n",
    "    print(f\"Runtime p50 {seconds_to_hms(run_p50)}   p90 {seconds_to_hms(run_p90)}   p95 {seconds_to_hms(run_p95)}\")\n",
    "    print(f\"Missing counts, creation NaN {preexisting:,}   scheduled NaN {prescheduled:,}   deletion NaN {posttrace:,}\")\n",
    "    if not conc_series.empty:\n",
    "        print(f\"Concurrency samples {len(conc_series):,} from tmin {tmin:.0f} to tmax {tmax:.0f} seconds\")\n",
    "    print(f\"Outputs are in {OUTDIR.resolve()}\")\n",
    "    print(\"======================================================\\n\")\n",
    "\n",
    "    # --------------------------- HTML report ---------------------------\n",
    "\n",
    "    figs = sorted(FIGDIR.glob(\"*.png\"))\n",
    "    tables = sorted([\n",
    "        OUTDIR / \"overall_resource_stats.csv\",\n",
    "        OUTDIR / \"per_role_resource_stats.csv\",\n",
    "        OUTDIR / \"correlation_matrix.csv\",\n",
    "        OUTDIR / \"per_app_aggregates.csv\",\n",
    "        OUTDIR / \"max_instance_per_node_counts.csv\",\n",
    "        OUTDIR / \"time_summaries.csv\",\n",
    "        OUTDIR / \"concurrency_over_time.csv\",\n",
    "        OUTDIR / \"arrivals_per_hour.csv\",\n",
    "        OUTDIR / \"departures_per_hour.csv\",\n",
    "    ], key=lambda p: p.name if p.exists() else \"zzz_\" + p.name)\n",
    "    tables = [p for p in tables if p.exists()]\n",
    "\n",
    "    context = {\n",
    "        \"n_rows\": n_rows,\n",
    "        \"n_cols\": n_cols,\n",
    "        \"n_apps\": n_apps,\n",
    "        \"cn\": cn,\n",
    "        \"hn\": hn,\n",
    "        \"gpu_share\": gpu_share,\n",
    "        \"cn_cpu_med\": cn_cpu_med,\n",
    "        \"cn_mem_med\": cn_mem_med,\n",
    "        \"cn_rdma_med\": cn_rdma_med,\n",
    "        \"hn_cpu_med\": hn_cpu_med,\n",
    "        \"hn_mem_med\": hn_mem_med,\n",
    "        \"hn_gpu_med\": hn_gpu_med,\n",
    "        \"hn_rdma_med\": hn_rdma_med,\n",
    "        \"corr_cpu_mem\": corr_cpu_mem,\n",
    "        \"corr_gpu_rdma\": corr_gpu_rdma,\n",
    "        \"ipn_mode\": ipn_mode if ipn_mode is not None else float(\"nan\"),\n",
    "        \"ipn_no_limit\": ipn_no_limit_share,\n",
    "        \"sched_p50\": seconds_to_hms(sched_p50),\n",
    "        \"sched_p90\": seconds_to_hms(sched_p90),\n",
    "        \"sched_p95\": seconds_to_hms(sched_p95),\n",
    "        \"run_p50\": seconds_to_hms(run_p50),\n",
    "        \"run_p90\": seconds_to_hms(run_p90),\n",
    "        \"run_p95\": seconds_to_hms(run_p95),\n",
    "        \"preexist\": preexisting,\n",
    "        \"prescheduled\": prescheduled,\n",
    "        \"posttrace\": posttrace,\n",
    "        \"figs\": figs,\n",
    "        \"tables\": tables,\n",
    "    }\n",
    "    write_report_html(context, OUTDIR / \"report.html\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8f491",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
